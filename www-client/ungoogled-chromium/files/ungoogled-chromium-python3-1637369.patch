From 7c5a5879d897dbe828854184db609dbe630a1407 Mon Sep 17 00:00:00 2001
From: Lei Lei <leilei@chromium.org>
Date: Tue, 11 Jun 2019 17:33:34 +0000
Subject: [PATCH] [client] Update client code to make it compatible with python3

I also updated utils/tools.py to use different way to get absolute
path to client folder when it is not running from a zip file,
otherwise unit tests will be failed, since it will be failed to
find six module in third_party folder.

Bug: 942720
Change-Id: Id414f10f1f417240c0b59b882364c56eab9f3a72
Reviewed-on: https://chromium-review.googlesource.com/c/infra/luci/luci-py/+/1637369
Commit-Queue: Lei Lei <leilei@chromium.org>
Reviewed-by: Marc-Antoine Ruel <maruel@chromium.org>
---

diff --git a/appengine/swarming/appengine_config.py b/appengine/swarming/appengine_config.py
index 7b8e8ad..f438577 100644
--- a/appengine/swarming/appengine_config.py
+++ b/appengine/swarming/appengine_config.py
@@ -27,6 +27,7 @@
   import sys
   # Disable AppEngine's sandbox.
   from google.appengine.tools.devappserver2.python import stubs
+
   stubs.FakeFile.is_file_accessible = staticmethod(lambda *_: True)
   sys.meta_path = []
   for i in os.listdir('.'):
diff --git a/client/auth.py b/client/auth.py
index a1e9791..84be2d9 100755
--- a/client/auth.py
+++ b/client/auth.py
@@ -121,9 +121,9 @@
   process_auth_options(parser, options)
   service = AuthService(options.service)
   if service.login(True):
-    print 'Logged in as \'%s\'.' % service.get_current_identity()
+    print('Logged in as \'%s\'.' % service.get_current_identity())
     return 0
-  print 'Login failed or canceled.'
+  print('Login failed or canceled.')
   return 1
 
 
@@ -144,7 +144,7 @@
   process_auth_options(parser, options)
   service = AuthService(options.service)
   service.login(False)
-  print service.get_current_identity()
+  print(service.get_current_identity())
   return 0
 
 
diff --git a/client/cipd.py b/client/cipd.py
index a4ef49a..52a310b 100644
--- a/client/cipd.py
+++ b/client/cipd.py
@@ -434,7 +434,7 @@
     file_path.ensure_tree(cipd_bin_dir)
 
   with instance_cache.getfileobj(instance_id) as f:
-    isolateserver.putfile(f, binary_path, 0511)  # -r-x--x--x
+    isolateserver.putfile(f, binary_path, 0o511)  # -r-x--x--x
 
   _ensure_batfile(binary_path)
 
diff --git a/client/isolated_format.py b/client/isolated_format.py
index 39ab37a..37cee20 100644
--- a/client/isolated_format.py
+++ b/client/isolated_format.py
@@ -16,7 +16,6 @@
 from utils import fs
 from utils import tools
 
-
 # Version stored and expected in .isolated files.
 ISOLATED_FILE_VERSION = '1.6'
 
@@ -35,7 +34,7 @@
 
 
 # Used for serialization.
-SUPPORTED_ALGOS_REVERSE = dict((v, k) for k, v in SUPPORTED_ALGOS.iteritems())
+SUPPORTED_ALGOS_REVERSE = dict((v, k) for k, v in SUPPORTED_ALGOS.items())
 
 
 SUPPORTED_FILE_TYPES = ['basic', 'tar']
diff --git a/client/isolateserver.py b/client/isolateserver.py
index a518914..7953ade 100755
--- a/client/isolateserver.py
+++ b/client/isolateserver.py
@@ -13,7 +13,6 @@
 import logging
 import optparse
 import os
-import Queue
 import re
 import signal
 import stat
@@ -30,6 +29,7 @@
 import colorama
 from depot_tools import fix_encoding
 from depot_tools import subcommand
+from six.moves import queue as Queue
 
 # pylint: disable=ungrouped-imports
 import auth
@@ -538,7 +538,8 @@
     """ThreadPool for CPU-bound tasks like zipping."""
     if self._cpu_thread_pool is None:
       threads = max(threading_utils.num_processors(), 2)
-      if sys.maxsize <= 2L**32:
+      max_size = long(2)**32 if sys.version_info.major == 2 else 2**32
+      if sys.maxsize <= max_size:
         # On 32 bits userland, do not try to use more than 16 threads.
         threads = min(threads, 16)
       self._cpu_thread_pool = threading_utils.ThreadPool(2, threads, 0, 'zip')
@@ -1236,10 +1237,10 @@
 
     if filetype == 'basic':
       # Ignore all bits apart from the user.
-      file_mode = (props.get('m') or 0500) & 0700
+      file_mode = (props.get('m') or 0o500) & 0o700
       if read_only:
         # Enforce read-only if the root bundle does.
-        file_mode &= 0500
+        file_mode &= 0o500
       putfile(srcfileobj, dst, file_mode,
               use_symlink=use_symlinks)
 
@@ -1263,10 +1264,10 @@
                 fp)
           ifd = t.extractfile(ti)
           file_path.ensure_tree(os.path.dirname(fp))
-          file_mode = ti.mode & 0700
+          file_mode = ti.mode & 0o700
           if read_only:
             # Enforce read-only if the root bundle does.
-            file_mode &= 0500
+            file_mode &= 0o500
           putfile(ifd, fp, file_mode, ti.size)
 
     else:
diff --git a/client/libs/luci_context/__init__.py b/client/libs/luci_context/__init__.py
index e601053..4414dcf 100644
--- a/client/libs/luci_context/__init__.py
+++ b/client/libs/luci_context/__init__.py
@@ -3,4 +3,4 @@
 # that can be found in the LICENSE file.
 
 # pylint: disable=relative-import
-from luci_context import read_full, read, write, stage
+from .luci_context import read_full, read, write, stage
diff --git a/client/local_caching.py b/client/local_caching.py
index f5e7213..283c811 100644
--- a/client/local_caching.py
+++ b/client/local_caching.py
@@ -19,7 +19,10 @@
 from utils import lru
 from utils import threading_utils
 from utils import tools
+tools.force_local_third_party()
 
+# third_party/
+import six
 
 # The file size to be used when we don't know the correct file size,
 # generally used for .isolated files.
@@ -89,7 +92,7 @@
       oldest = [(c, c.get_oldest()) for c in caches if len(c) > 0]
       if not oldest:
         break
-      oldest.sort(key=lambda (_, ts): ts)
+      oldest.sort(key=lambda k: k[1])
       c, ts = oldest[0]
       if ts >= min_ts and free_disk >= min_free_space:
         break
@@ -302,7 +305,7 @@
 class MemoryContentAddressedCache(ContentAddressedCache):
   """ContentAddressedCache implementation that stores everything in memory."""
 
-  def __init__(self, file_mode_mask=0500):
+  def __init__(self, file_mode_mask=0o500):
     """Args:
       file_mode_mask: bit mask to AND file mode with. Default value will make
           all mapped files to be read only.
@@ -476,16 +479,16 @@
     policies.
     """
     with self._lock:
-      fs.chmod(self.cache_dir, 0700)
+      fs.chmod(self.cache_dir, 0o700)
       # Ensure that all files listed in the state still exist and add new ones.
       previous = set(self._lru)
       # It'd be faster if there were a readdir() function.
       for filename in fs.listdir(self.cache_dir):
         if filename == self.STATE_FILE:
-          fs.chmod(os.path.join(self.cache_dir, filename), 0600)
+          fs.chmod(os.path.join(self.cache_dir, filename), 0o600)
           continue
         if filename in previous:
-          fs.chmod(os.path.join(self.cache_dir, filename), 0400)
+          fs.chmod(os.path.join(self.cache_dir, filename), 0o400)
           previous.remove(filename)
           continue
 
@@ -852,7 +855,7 @@
         # Raise using the original traceback.
         exc = NamedCacheError(
             'cannot install cache named %r at %r: %s' % (name, dst, ex))
-        raise exc, None, sys.exc_info()[2]
+        six.reraise(exc, None, sys.exc_info()[2])
       finally:
         self._save()
 
@@ -925,7 +928,7 @@
         # Raise using the original traceback.
         exc = NamedCacheError(
             'cannot uninstall cache named %r at %r: %s' % (name, src, ex))
-        raise exc, None, sys.exc_info()[2]
+        six.reraise(exc, None, sys.exc_info()[2])
       finally:
         # Call save() at every uninstall. The assumptions are:
         # - The total the number of named caches is low, so the state.json file
diff --git a/client/run_isolated.py b/client/run_isolated.py
index a45fe09..e2cd52c 100755
--- a/client/run_isolated.py
+++ b/client/run_isolated.py
@@ -106,7 +106,7 @@
 
 
 # Keep synced with task_request.py
-CACHE_NAME_RE = re.compile(ur'^[a-z0-9_]{1,4096}$')
+CACHE_NAME_RE = re.compile(r'^[a-z0-9_]{1,4096}$')
 
 
 OUTLIVING_ZOMBIE_MSG = """\
@@ -672,7 +672,7 @@
   }
 
   if data.root_dir:
-    file_path.ensure_tree(data.root_dir, 0700)
+    file_path.ensure_tree(data.root_dir, 0o700)
   elif data.isolate_cache.cache_dir:
     data = data._replace(
         root_dir=os.path.dirname(data.isolate_cache.cache_dir))
@@ -684,7 +684,7 @@
     run_dir = os.path.join(data.root_dir, ISOLATED_RUN_DIR)
     if os.path.isdir(run_dir):
       file_path.rmtree(run_dir)
-    os.mkdir(run_dir, 0700)
+    os.mkdir(run_dir, 0o700)
   else:
     run_dir = make_temp_dir(ISOLATED_RUN_DIR, data.root_dir)
   # storage should be normally set but don't crash if it is not. This can happen
@@ -737,7 +737,7 @@
 
       if not os.path.isdir(cwd):
         # Accepts relative_cwd that does not exist.
-        os.makedirs(cwd, 0700)
+        os.makedirs(cwd, 0o700)
 
       # If we have an explicit list of files to return, make sure their
       # directories exist now.
diff --git a/client/swarming.py b/client/swarming.py
index 8e6638b..ca1dc35 100755
--- a/client/swarming.py
+++ b/client/swarming.py
@@ -5,7 +5,9 @@
 
 """Client tool to trigger tasks or retrieve results from a Swarming server."""
 
-__version__ = '0.14'
+from __future__ import print_function
+
+__version__ = '1.0'
 
 import collections
 import datetime
@@ -295,8 +297,8 @@
     if (not priority_warning and
         int(task['request']['priority']) != task_request.priority):
       priority_warning = True
-      print >> sys.stderr, (
-          'Priority was reset to %s' % task['request']['priority'])
+      print('Priority was reset to %s' % task['request']['priority'],
+            file=sys.stderr)
     tasks[request['name']] = {
       'shard_index': shard_index,
       'task_id': task['task_id'],
@@ -306,8 +308,8 @@
   # Some shards weren't triggered. Abort everything.
   if len(tasks) != len(requests):
     if tasks:
-      print >> sys.stderr, 'Only %d shard(s) out of %d were triggered' % (
-          len(tasks), len(requests))
+      print('Only %d shard(s) out of %d were triggered' % (
+          len(tasks), len(requests)), file=sys.stderr)
       for task_dict in tasks.itervalues():
         abort_task(swarming, task_dict['task_id'])
     return None
@@ -439,7 +441,7 @@
         isolateserver.fetch_isolated(
             result['outputs_ref']['isolated'],
             storage,
-            local_caching.MemoryContentAddressedCache(file_mode_mask=0700),
+            local_caching.MemoryContentAddressedCache(file_mode_mask=0o700),
             os.path.join(self.task_output_dir, str(shard_index)),
             False, self.filter_cb)
 
@@ -806,8 +808,8 @@
 
   if len(seen_shards) != len(task_ids):
     missing_shards = [x for x in range(len(task_ids)) if x not in seen_shards]
-    print >> sys.stderr, ('Results from some shards are missing: %s' %
-        ', '.join(map(str, missing_shards)))
+    print('Results from some shards are missing: %s' %
+        ', '.join(map(str, missing_shards)), file=sys.stderr)
     return 1
 
   return exit_code if exit_code is not None else 1
@@ -1407,12 +1409,12 @@
     sys.stderr.write('\n%s\n' % e)
     return 1
   for bot in natsort.natsorted(bots, key=lambda x: x['bot_id']):
-    print bot['bot_id']
+    print(bot['bot_id'])
     if not options.bare:
       dimensions = {i['key']: i.get('value') for i in bot.get('dimensions', {})}
-      print '  %s' % json.dumps(dimensions, sort_keys=True)
+      print('  %s' % json.dumps(dimensions, sort_keys=True))
       if bot.get('task_id'):
-        print '  task: %s' % bot['task_id']
+        print('  task: %s' % bot['task_id'])
   return 0
 
 
@@ -1611,8 +1613,8 @@
     for i, (api_id, api) in enumerate(sorted(apis.iteritems())):
       if i:
         print('')
-      print api_id
-      print '  ' + api['description'].strip()
+      print(api_id)
+      print('  ' + api['description'].strip())
       if 'resources' in api:
         # Old.
         # TODO(maruel): Remove.
@@ -1625,22 +1627,22 @@
             # Only list the GET ones.
             if method['httpMethod'] != 'GET':
               continue
-            print '- %s.%s: %s' % (
-                resource_name, method_name, method['path'])
+            print('- %s.%s: %s' % (
+                resource_name, method_name, method['path']))
             print('\n'.join(
                 '  ' + l for l in textwrap.wrap(
                     method.get('description', 'No description'), 78)))
-            print '  %s%s%s' % (help_url, api['servicePath'], method['id'])
+            print('  %s%s%s' % (help_url, api['servicePath'], method['id']))
       else:
         # New.
         for method_name, method in sorted(api['methods'].iteritems()):
           # Only list the GET ones.
           if method['httpMethod'] != 'GET':
             continue
-          print '- %s: %s' % (method['id'], method['path'])
+          print('- %s: %s' % (method['id'], method['path']))
           print('\n'.join(
               '  ' + l for l in textwrap.wrap(method['description'], 78)))
-          print '  %s%s%s' % (help_url, api['servicePath'], method['id'])
+          print('  %s%s%s' % (help_url, api['servicePath'], method['id']))
   return 0
 
 
@@ -1738,7 +1740,7 @@
   url = options.swarming + '/_ah/api/swarming/v1/task/%s/request' % args[0]
   request = net.url_read_json(url)
   if not request:
-    print >> sys.stderr, 'Failed to retrieve request data for the task'
+    print('Failed to retrieve request data for the task', file=sys.stderr)
     return 1
 
   workdir = unicode(os.path.abspath(options.work))
@@ -1831,8 +1833,8 @@
   try:
     return subprocess42.call(command + extra_args, env=env, cwd=workdir)
   except OSError as e:
-    print >> sys.stderr, 'Failed to run: %s' % ' '.join(command)
-    print >> sys.stderr, str(e)
+    print('Failed to run: %s' % ' '.join(command), file=sys.stderr)
+    print(str(e), file=sys.stderr)
     return 1
   finally:
     # Do not delete options.cache.
@@ -1855,7 +1857,7 @@
   url = options.swarming + '/_ah/api/swarming/v1/bot/%s/terminate' % args[0]
   request = net.url_read_json(url, data={})
   if not request:
-    print >> sys.stderr, 'Failed to ask for termination'
+    print('Failed to ask for termination', file=sys.stderr)
     return 1
   if options.wait:
     return collect(
@@ -1870,7 +1872,7 @@
         False,
         None)
   else:
-    print request['task_id']
+    print(request['task_id'])
   return 0
 
 
diff --git a/client/tests/logging_utils/phase1.py b/client/tests/logging_utils/phase1.py
index e0f7cb5..38fa5f1 100755
--- a/client/tests/logging_utils/phase1.py
+++ b/client/tests/logging_utils/phase1.py
@@ -11,6 +11,7 @@
 THIS_DIR = os.path.dirname(os.path.abspath(__file__))
 CLIENT_DIR = os.path.dirname(os.path.dirname(THIS_DIR))
 sys.path.insert(0, CLIENT_DIR)
+sys.path.insert(0, os.path.join(CLIENT_DIR, 'third_party'))
 
 from utils import logging_utils
 
diff --git a/client/tests/logging_utils/phase2.py b/client/tests/logging_utils/phase2.py
index 47be802..c24ab66 100755
--- a/client/tests/logging_utils/phase2.py
+++ b/client/tests/logging_utils/phase2.py
@@ -10,6 +10,7 @@
 THIS_DIR = os.path.dirname(os.path.abspath(__file__))
 CLIENT_DIR = os.path.dirname(os.path.dirname(THIS_DIR))
 sys.path.insert(0, CLIENT_DIR)
+sys.path.insert(0, os.path.join(CLIENT_DIR, 'third_party'))
 
 from utils import logging_utils
 
diff --git a/client/utils/file_path.py b/client/utils/file_path.py
index cdcf5fc..685cae6 100644
--- a/client/utils/file_path.py
+++ b/client/utils/file_path.py
@@ -25,7 +25,10 @@
 from utils import fs
 from utils import subprocess42
 from utils import tools
+tools.force_local_third_party()
 
+# third_party/
+import six
 
 # Types of action accepted by link_file().
 HARDLINK, HARDLINK_WITH_FALLBACK, SYMLINK, SYMLINK_WITH_FALLBACK, COPY = range(
@@ -241,7 +244,7 @@
     # the set_read_only() call to remove the read only bit had silently failed
     # because there was no DACL for the user.
     if not (os.stat(path).st_mode & stat.S_IWUSR):
-      os.chmod(path, 0777)
+      os.chmod(path, 0o777)
 
 
   def isabs(path):
@@ -293,7 +296,7 @@
     # Go figure why GetShortPathName() is needed.
     try:
       out = GetLongPathName(GetShortPathName(p))
-    except OSError, e:
+    except OSError as e:
       if e.args[0] in (2, 3, 5):
         # The path does not exist. Try to recurse and reconstruct the path.
         base = os.path.dirname(p)
@@ -581,7 +584,7 @@
       if p.endswith(os.path.sep) and not out.endswith(os.path.sep):
         return out + os.path.sep
       return out
-    except MacOS.Error, e:
+    except MacOS.Error as e:
       if e.args[0] in (-43, -120):
         # The path does not exist. Try to recurse and reconstruct the path.
         # -43 means file not found.
@@ -830,8 +833,8 @@
     # If the drive letter mismatches, assume it's a separate partition.
     # TODO(maruel): It should look at the underlying drive, a drive letter could
     # be a mount point to a directory on another drive.
-    assert re.match(ur'^[a-zA-Z]\:\\.*', path1), path1
-    assert re.match(ur'^[a-zA-Z]\:\\.*', path2), path2
+    assert re.match(r'^[a-zA-Z]\:\\.*', path1), path1
+    assert re.match(r'^[a-zA-Z]\:\\.*', path2), path2
     if path1[0].lower() != path2[0].lower():
       return False
   return fs.stat(path1).st_dev == fs.stat(path2).st_dev
@@ -1031,7 +1034,7 @@
 ### Write directory functions.
 
 
-def ensure_tree(path, perm=0777):
+def ensure_tree(path, perm=0o777):
   """Ensures a directory exists."""
   if not fs.isdir(path):
     try:
@@ -1216,7 +1219,7 @@
 
   # If soft retries fail on Linux, there's nothing better we can do.
   if sys.platform != 'win32':
-    raise errors[0][2][0], errors[0][2][1], errors[0][2][2]
+    six.reraise(errors[0][2][0], errors[0][2][1], errors[0][2][2])
 
   # The soft way was not good enough. Try the hard way.
   for i in xrange(max_tries):
@@ -1228,7 +1231,7 @@
     processes = _get_children_processes_win(root)
     if processes:
       sys.stderr.write('Failed to terminate processes.\n')
-      raise errors[0][2][0], errors[0][2][1], errors[0][2][2]
+      six.reraise(errors[0][2][0], errors[0][2][1], errors[0][2][2])
 
   # Now that annoying processes in root are evicted, try again.
   errors = []
@@ -1241,7 +1244,7 @@
     # The same path may be listed multiple times.
     for path in sorted(set(path for _, path, _ in errors)):
       sys.stderr.write('- %s\n' % path)
-    raise errors[0][2][0], errors[0][2][1], errors[0][2][2]
+    six.reraise(errors[0][2][0], errors[0][2][1], errors[0][2][2])
   return False
 
 
diff --git a/client/utils/fs.py b/client/utils/fs.py
index e04b375..f23ded5 100644
--- a/client/utils/fs.py
+++ b/client/utils/fs.py
@@ -4,7 +4,6 @@
 
 """Wraps os, os.path and shutil functions to work around MAX_PATH on Windows."""
 
-import __builtin__
 import inspect
 import os
 import re
@@ -12,6 +11,11 @@
 import subprocess
 import sys
 
+from utils import tools
+tools.force_local_third_party()
+
+# third_party/
+from six.moves import builtins
 
 if sys.platform == 'win32':
   import ctypes
@@ -392,7 +396,7 @@
 
 
 def open(path, *args, **kwargs):  # pylint: disable=redefined-builtin
-  return __builtin__.open(extend(path), *args, **kwargs)
+  return builtins.open(extend(path), *args, **kwargs)
 
 
 ## os
diff --git a/client/utils/large.py b/client/utils/large.py
index 90bc7f6..f001d3b 100644
--- a/client/utils/large.py
+++ b/client/utils/large.py
@@ -11,7 +11,7 @@
 can be very high for monotonically increasing sets.
 """
 
-import cStringIO
+import sys
 import zlib
 
 
@@ -28,7 +28,7 @@
   if not values:
     return ''
   last = 0
-  max_value = 2L**63
+  max_value = long(2)**63 if sys.version_info.major == 2 else 2**63
   assert 0 <= values[0] < max_value, 'Values must be between 0 and 2**63'
   assert 0 <= values[-1] < max_value, 'Values must be between 0 and 2**63'
   for value in values:
diff --git a/client/utils/net.py b/client/utils/net.py
index c257386..462483d 100644
--- a/client/utils/net.py
+++ b/client/utils/net.py
@@ -4,7 +4,6 @@
 
 """Classes and functions for generic network communication over HTTP."""
 
-import cookielib
 import itertools
 import json
 import logging
@@ -14,10 +13,21 @@
 import re
 import socket
 import ssl
+import sys
 import threading
 import time
 import urllib
-import urlparse
+# pylint: disable=ungrouped-imports
+# pylint: disable=no-name-in-module
+if sys.version_info.major == 2:
+  import urlparse
+  urlencode = urllib.urlencode
+else:
+  from urllib import parse as urlparse
+  urlencode = urlparse.urlencode
+
+from utils import tools
+tools.force_local_third_party()
 
 # third_party/
 import requests
@@ -25,6 +35,7 @@
 from requests import structures
 import urllib3
 
+from six.moves import http_cookiejar as cookielib
 from utils import authenticators
 from utils import oauth
 from utils import tools
@@ -57,7 +68,7 @@
 # Content type -> function that encodes a request body.
 CONTENT_ENCODERS = {
   URL_ENCODED_FORM_CONTENT_TYPE:
-    urllib.urlencode,
+    urlencode,
   JSON_CONTENT_TYPE:
     lambda x: json.dumps(x, sort_keys=True, separators=(',', ':')),
 }
diff --git a/client/utils/oauth.py b/client/utils/oauth.py
index 19da836..a28f186 100644
--- a/client/utils/oauth.py
+++ b/client/utils/oauth.py
@@ -4,10 +4,11 @@
 
 """OAuth2 related utilities and implementation of browser based login flow."""
 
+from __future__ import print_function
+
 # pylint: disable=W0613
 
 import base64
-import BaseHTTPServer
 import collections
 import datetime
 import json
@@ -19,9 +20,17 @@
 import threading
 import time
 import urllib
-import urlparse
+# pylint: disable=ungrouped-imports
+# pylint: disable=no-name-in-module
+if sys.version_info.major == 2:
+  import urlparse # Python 2
+else:
+  from urllib import parse as urlparse # Python 3
 import webbrowser
 
+from utils import tools
+tools.force_local_third_party()
+
 # third_party/
 import httplib2
 import rsa
@@ -33,7 +42,7 @@
 import requests
 
 from libs import luci_context
-from utils import tools
+from six.moves import BaseHTTPServer
 
 
 # Path to a file with cached OAuth2 credentials used by default. Can be
@@ -550,9 +559,9 @@
 def _run_oauth_dance(urlhost, config):
   """Perform full OAuth2 dance with the browser."""
   def out(s):
-    print s
+    print(s)
   def err(s):
-    print >> sys.stderr, s
+    print(s, file=sys.stderr)
 
   # Fetch client_id and client_secret from the service itself.
   service_config = _fetch_service_config(urlhost)
diff --git a/client/utils/subprocess42.py b/client/utils/subprocess42.py
index 8806b5f..dbc0d36 100644
--- a/client/utils/subprocess42.py
+++ b/client/utils/subprocess42.py
@@ -50,7 +50,7 @@
 _OS_ERROR_REPORTING_INHIBITED = False
 
 
-if subprocess.mswindows:
+if sys.platform == 'win32':
   import ctypes
   import msvcrt  # pylint: disable=F0401
   from ctypes import wintypes
@@ -497,7 +497,7 @@
 
     self.detached = kwargs.pop('detached', False)
     if self.detached:
-      if subprocess.mswindows:
+      if sys.platform == 'win32':
         prev = kwargs.get('creationflags', 0)
         kwargs['creationflags'] = prev | CREATE_NEW_PROCESS_GROUP
       else:
@@ -509,7 +509,7 @@
         kwargs['preexec_fn'] = new_preexec_fn_1
 
     if kwargs.pop('lower_priority', False):
-      if subprocess.mswindows:
+      if sys.platform == 'win32':
         # TODO(maruel): If already in this class, it should use
         # IDLE_PRIORITY_CLASS.
         prev = kwargs.get('creationflags', 0)
@@ -525,10 +525,10 @@
     self.containment = kwargs.pop('containment', None) or Containment()
     if self.containment.containment_type != Containment.NONE:
       if self.containment.containment_type == Containment.JOB_OBJECT:
-        if not subprocess.mswindows:
+        if sys.platform != 'win32':
           raise NotImplementedError(
               'containment is not implemented on this platform')
-      if subprocess.mswindows:
+      if sys.platform == 'win32':
         # May throw an WindowsError.
         # pylint: disable=undefined-variable
         self._job = _JobObject(self.containment)
@@ -542,7 +542,7 @@
     self.start = time.time()
     try:
       with self.popen_lock:
-        if subprocess.mswindows:
+        if sys.platform == 'win32':
           # We need the thread handle, save it.
           old = subprocess._subprocess.CreateProcess
           class FakeHandle(object):
@@ -558,14 +558,14 @@
         try:
           super(Popen, self).__init__(args, **kwargs)
         finally:
-          if subprocess.mswindows:
+          if sys.platform == 'win32':
             subprocess._subprocess.CreateProcess = old
     except:
       self._cleanup()
       raise
 
     self.args = args
-    if self.detached and not subprocess.mswindows:
+    if self.detached and sys.platform != 'win32':
       try:
         self.gid = os.getpgid(self.pid)
       except OSError:
@@ -670,7 +670,7 @@
     if timeout is None:
       super(Popen, self).wait()
     elif self.returncode is None:
-      if subprocess.mswindows:
+      if sys.platform == 'win32':
         WAIT_TIMEOUT = 258
         result = subprocess._subprocess.WaitForSingleObject(
             self._handle, int(timeout * 1000))
@@ -848,7 +848,7 @@
     On Posix, always send SIGTERM.
     """
     try:
-      if subprocess.mswindows and self.detached:
+      if sys.platform == 'win32' and self.detached:
         return self.send_signal(signal.CTRL_BREAK_EVENT)
       super(Popen, self).terminate()
     except OSError:
diff --git a/client/utils/threading_utils.py b/client/utils/threading_utils.py
index 5f7b124..c38c8ae 100644
--- a/client/utils/threading_utils.py
+++ b/client/utils/threading_utils.py
@@ -8,12 +8,18 @@
 import inspect
 import logging
 import os
-import Queue
 import sys
 import threading
 import time
 import traceback
 
+from utils import tools
+tools.force_local_third_party()
+
+# third_party/
+import six
+from six.moves import queue as Queue
+
 
 # Priorities for tasks in AutoRetryThreadPool, particular values are important.
 PRIORITY_HIGH = 1 << 8
@@ -235,7 +241,7 @@
     with self._outputs_exceptions_cond:
       if self._exceptions:
         e = self._exceptions.pop(0)
-        raise e[0], e[1], e[2]
+        six.reraise(e[0], e[1], e[2])
       out = self._outputs
       self._outputs = []
     return out
@@ -262,7 +268,7 @@
       with self._outputs_exceptions_cond:
         if self._exceptions:
           e = self._exceptions.pop(0)
-          raise e[0], e[1], e[2]
+          six.reraise(e[0], e[1], e[2])
         if self._outputs:
           # Remember the result to yield it outside of the lock.
           result = self._outputs.pop(0)
@@ -413,7 +419,7 @@
   """
   # Initial and maximum number of worker threads.
   INITIAL_WORKERS = 2
-  MAX_WORKERS = 16 if sys.maxsize > 2L**32 else 8
+  MAX_WORKERS = 16 if sys.maxsize > 2**32 else 8
   RETRIES = 5
 
   def __init__(self):
@@ -788,7 +794,7 @@
       # to preserve stack frame of original exception (that was raised in
       # another thread).
       assert isinstance(value, tuple) and len(value) == 3
-      raise value[0], value[1], value[2]
+      six.reraise(value[0], value[1], value[2])
     if item_type == self._ITEM_DONE:
       raise StopIteration()
     assert False, 'Impossible queue item type: %r' % item_type
diff --git a/client/utils/zip_package.py b/client/utils/zip_package.py
index 43e89d3..7692988 100755
--- a/client/utils/zip_package.py
+++ b/client/utils/zip_package.py
@@ -6,7 +6,6 @@
 
 import atexit
 import collections
-import cStringIO as StringIO
 import hashlib
 import os
 import pkgutil
@@ -17,6 +16,10 @@
 import zipfile
 import zipimport
 
+if sys.version_info.major == 2:
+  import StringIO
+else:
+  import io as StringIO
 
 # Glob patterns for files to exclude from a package by default.
 EXCLUDE_LIST = (
@@ -37,7 +40,7 @@
 
 # Patch zipimport.zipimporter hook to accept unicode strings
 def zipimporter_unicode(archivepath):
-  if isinstance(archivepath, unicode):
+  if sys.version_info.major == 2 and isinstance(archivepath, unicode):
     archivepath = archivepath.encode(sys.getfilesystemencoding())
   return zipimport.zipimporter(archivepath)
 
@@ -213,7 +216,7 @@
         info.compress_type = compression
         info.create_system = 3
         if isinstance(ref, ZipPackage._FileRef):
-          info.external_attr = (os.stat(ref.abs_path)[0] & 0xFFFF) << 16L
+          info.external_attr = (os.stat(ref.abs_path)[0] & 0xFFFF) << 16
           with open(ref.abs_path, 'rb') as f:
             buf = f.read()
         elif isinstance(ref, ZipPackage._BufferRef):
@@ -254,7 +257,9 @@
 
   path = getattr(main, '__file__', None)
   if path:
-    return path.decode(sys.getfilesystemencoding())
+    if sys.version_info.major == 2:
+      return path.decode(sys.getfilesystemencoding())
+    return path
 
 
 def _write_temp_data(name, data, temp_dir):
@@ -270,7 +275,7 @@
     return None
 
   try:
-    fd = os.open(filepath, os.O_WRONLY|os.O_CREAT|os.O_EXCL, 0600)
+    fd = os.open(filepath, os.O_WRONLY|os.O_CREAT|os.O_EXCL, 0o600)
     with os.fdopen(fd, 'wb') as f:
       f.write(data)
     return filepath
